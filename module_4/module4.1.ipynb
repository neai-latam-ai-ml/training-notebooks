{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a632af-3bb3-4a4a-8305-b0f94814bc83",
   "metadata": {},
   "source": [
    "# Hands On with Advanced AI & Emerging Applications\n",
    "\n",
    "## Requirementes\n",
    "\n",
    "* Definition of al Large Language Model (LLM)\n",
    "* Definitions of a Prompt\n",
    "\n",
    "## Retrieval Augmented Generation\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a technique that is used in natural language processing to enhance the capabilities of Large Language Models (LLMs) by integrating information retrieval. It improves the responses of LLMs by incorporating updated or user-defined data the LLM was originally not trained on. A RAG system is typically composed of two processes: Indexing and Retrieval + Generation.\n",
    "\n",
    "This module uses the [LangChain](https://www.langchain.com/) to interact with LLMs. Langchain is a framework designed to facititate the development of application powered by LLMs. It provides an abstraction layer by decompossing applications as a set of _chains_ made of prompts, models and data sources. Langchain offers libraries to interact with most of the foundational models available in the market.\n",
    "\n",
    "In this module we will use [Amazon Bedrock](https://aws.amazon.com/bedrock/) which is a fully managed service from AWS that offers access to several foundationsl models. We have chosen [Cloude Sonnet](https://www.anthropic.com/claude/sonnet) from Anthropic as the LLM to use\n",
    "\n",
    "### Setting up connection to Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fd77e-a99c-4501-a70f-c59669244210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langchain-community langchain_aws boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7593be-49b8-4c5c-a581-52de07477ccc",
   "metadata": {},
   "source": [
    "**Note**: Use this if running from Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf37177-de9e-4f02-966a-1fb0ddccf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=\"your-key-id\",\n",
    "    aws_secret_access_key=\"your-key\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    client=session.client(\"bedrock-runtime\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07de18e-b7fd-4661-897e-7bec2872c38d",
   "metadata": {},
   "source": [
    "**Note**: Use this if running from AWS directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48fc8a-b7b7-4fb2-903f-a13f6847d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55d523-2a47-419d-9350-ce64f4372cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = llm.invoke(\"Who is the president of the USA?\")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86dd86d-6561-4ae8-bab3-643174b2bca5",
   "metadata": {},
   "source": [
    "**Note**: See how Claude Sonnet, an LLM released in February 2024, was trained on data that is no longer accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105cb8d-caf4-49ba-a5b3-362e42b8cd2d",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "The indexing phase is responsible for organizing data so that it can be efficiently retrieved by applications powered by large language models (LLMs). To scale Retrieval-Augmented Generation (RAG) systems and handle large datasets, vector databases are commonly used. These databases store information and facilitate efficient data indexing and querying\n",
    "\n",
    "For the purpose of this section we are going to load the content of the markdown file `acme_guidelines.md`. This document has guidelines of the ficitonal companny `ACME` about the IP addresses schemes to be used in the device configuration.\n",
    "\n",
    "\n",
    "LanhChain offers a vast number of classes to load documents, ranging from text files (`.txt`) up to HTML and PDF documents. To load the `acme_guidelines.md` file we are goint ot use the class `TextLoader` from the `langchain.document_loaders` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab76b73-6e60-4700-8108-1533de1c3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"acme_guidelines.md\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f2712-a115-451b-80f3-345b6df3f8bc",
   "metadata": {},
   "source": [
    "#### Splitting\n",
    "As a best practice, data to be stored on the Vertor DB is usually splitted into smaller _chunks_ of data. This makes the _querying_ stage more efficient as only relevant pieces of the data are retrieved. Additionally large chunks may latter not fit in the LLM context window. \n",
    "\n",
    "There are many strategies to split a text. For example, to consider special characters (HTML tags) or punctiation signs. In our example we are using yet another Langchain Class to split a text. The `MarkdownHeaderTextSplitter` allows us to split markdown files based on _Headers_ or _sections_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985f2f9-05ee-4d92-9085-afc3ef9aa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "md_splits = md_splitter.split_text(documents[0].page_content)\n",
    "md_splits[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9aeb2-6f74-40ba-bf44-99e2da787ecd",
   "metadata": {},
   "source": [
    "Another alternative to split text files is to use the class `RecursiveCharacterTextSplitter`, which allows us to define separators between chunks, the size of each chunk and the number of items being overlapped between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56e3f8-aaf1-4cf4-a1b1-82b9d3f2b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\"], \n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e3aac-6a5f-438a-8972-7b3bf5ab2820",
   "metadata": {},
   "source": [
    "At this point we can test the content of a chunk by displaying the attribute `page_content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3aeb9-fe82-4c3d-9ffc-43c2b0adee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[3].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d57a6-0591-4364-a3b9-9eb6f864aa60",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "The next and last stage in the _Indexing_ process is to actually store the chunks in a Vector DB\n",
    "\n",
    "As the name implies, a Vector DB saved data as **vectors** not as raw text, therefore and embedding function must transform thus chunks into vectors. In this task we are using an _In-Memory_ Database and [Amazon Titan Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6940f74-618c-4ac3-bc17-7a86127dcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_aws import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e15d57-7f92-42c6-99f8-1ca1a4f5ef2c",
   "metadata": {},
   "source": [
    "**Note**: Use this if running from Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f0849-c191-4c00-8b39-5c994f383de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\", \n",
    "    client=session.client(\"bedrock-runtime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e735865-cf27-4f56-bc16-d5a62b653bcb",
   "metadata": {},
   "source": [
    "**Note**: Use this if running from AWS directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d264012-e283-4581-8cac-e7fb8f83b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06a289-d366-4ad7-875c-0ba7227f6691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(documents=md_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db6439-ed9e-4656-8530-30184a84c601",
   "metadata": {},
   "source": [
    "Note that we have used the variable `md_splits` which has the chunks generated by the `MarkdownHeaderTextSplitter`\n",
    "\n",
    "Let us examine the first two entries on the Vector DB. Note how the dimension of the embeddings in 1024. Only the first three items of each vector are being displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83af93-22fa-417c-8b46-2e47f2e915a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "    if index < 2:\n",
    "        print(f\"{id}: {doc['vector'][0:3]} ({len(doc['vector'])})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b68b7d-9fcd-4fb7-b165-c23cb05925be",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "Before actually implement the retrieval stage with the LLM, let us test the _querying_ of the vector VB with the method `similarity_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef27ba-b524-4dcb-942b-6b76a6682f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"approved security mechanisms\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06019dc-aa6f-49c4-ab15-f129ecbdf729",
   "metadata": {},
   "source": [
    "**Note:** See how the similary search takes into account semantic meaning of the text rather than exact words or grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac728116-f0d9-4a3b-a83d-1331e1521d88",
   "metadata": {},
   "source": [
    "### Retrieval Using LangChain LCEL\n",
    "\n",
    "In this task, we are going to use LangChain constructs, namely _chains_,  to test the capabilities of the RAG system. As a first step we will invoke the LLMs without using RAG just to realize the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335471de-0650-4dff-a176-b5facf353696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "query = \"What IPs are allowed in Ethernet Interfaces?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc596ff8-75bb-4ced-aa1f-baabf952133e",
   "metadata": {},
   "source": [
    "#### Query without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297034f-59ff-49d4-ae43-b88618d7c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = PromptTemplate.from_template(template = query) |  llm\n",
    "result = chain.invoke(input={})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442d52f-c347-417f-8408-727742fa0884",
   "metadata": {},
   "source": [
    "See how using basic prompt and invoking the LLM directly, the generated response is very generic. The LLM claims that any IP address can be used on a Ethernet Interface\n",
    "\n",
    "#### Querying with RAG\n",
    "\n",
    "The idea behind RAG is to provider more context to LLM, so it was use it to generate a response. In our current example the context are the Policies listed in the `acme_guidelines.md` document. We will use a  prompt that apart from specify the question to the LLM also give context to it\n",
    "```\n",
    "HUMAN\n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629c044-e64b-40b1-b1f9-62698077766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "retrieval_qa_chat_prompt.messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2cb766-a2a8-4386-aa38-e886aa3c1aa2",
   "metadata": {},
   "source": [
    "Note how the `rag_chain` is made from three components:\n",
    "\n",
    "1. Two parallel branch to get (a) the `context` and (b) `the question`\n",
    "2. The RAG promp\n",
    "3. The LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ae0ce-a8ab-4c18-b385-b0a68fcb8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "rag_chain = (\n",
    "    {'context': vector_store.as_retriever() | format_docs, 'question' : RunnablePassthrough()} |\n",
    "    retrieval_qa_chat_prompt |\n",
    "    llm\n",
    ")\n",
    "result = rag_chain.invoke(\"What IPs are allowed on Ethernet Interfaces\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b03fa-1b6a-4f93-9442-76fc3a19c621",
   "metadata": {},
   "source": [
    "See how in this execution the LLM considered the content of the `acme_guidelines.md` markdown file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c6aef-5323-4a58-b011-5502b8435b18",
   "metadata": {},
   "source": [
    "### Retrieval Using LangGraph\n",
    "\n",
    "In this task, we are going to use [LangGraph](https://www.langchain.com/langgraph) constructs, namely Nodes and Edges, to test the capabilities of the RAG\n",
    "\n",
    "**Note**: This task asummes that the in-memory VectorDB has already been indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efbc1f-57bb-4eef-8ac3-f770a680f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291975b-48f8-4f20-991b-f6593ace9b10",
   "metadata": {},
   "source": [
    "We start defining the _State_ of the grpah. The _State_ is the data being exchanged by the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef56b66-6cb0-45eb-8614-dfaa813be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94ca40-8a00-4cfa-9e1b-11667c5801fd",
   "metadata": {},
   "source": [
    "In our use-case we define two nodes: \n",
    "\n",
    "*  Retrieve: It queries the Vector DB using limiraty search from the input question\n",
    "*  Generate. It leverages the RAG prompt to give context to the LLM and expand its scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5524f-841e-4b61-88bd-a8be255e9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"],k=2)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = retrieval_qa_chat_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66350297-4271-4788-83e4-dc3ccc471f76",
   "metadata": {},
   "source": [
    "Then we build our graph by defining node and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def81e6b-8e84-45d1-ba44-d73001acac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ff0df-9ca5-4de6-9cff-ef2cee244b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2c769-b166-4f90-b048-69460d030446",
   "metadata": {},
   "source": [
    "Finally we invoke our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834948e6-284c-4368-bcad-7863fc314724",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"What IPs are allowed on Ethernet Interfaces\"})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef72c1-e512-4372-99d7-2df6891b56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f631c29-a6a8-4785-a378-18c72e4d1547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
