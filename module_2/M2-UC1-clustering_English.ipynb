{"cells":[{"cell_type":"markdown","source":["# **UC1. What different types of SD-WAN devices do I have deployed?**"],"metadata":{"id":"XzQ81MXCXyh5"},"id":"XzQ81MXCXyh5"},{"cell_type":"markdown","source":[" Managing thousands of network and cloud entities requires distinguishing roles and configurations. Identifying similarities and differences in data helps optimize operations, especially for users or subscribers."],"metadata":{"id":"iSxFnyhCYmNz"},"id":"iSxFnyhCYmNz"},{"cell_type":"markdown","source":["Clustering techniques group similar entities based on specific attributes, identifying patterns automatically. This helps optimize operational decisions by organizing entities with common characteristics."],"metadata":{"id":"vH7nLCcKY6M2"},"id":"vH7nLCcKY6M2"},{"cell_type":"markdown","source":["# Connection to drive and path definition (Just for Google Colab Lab)"],"metadata":{"id":"xs8t2_aS2eK4"},"id":"xs8t2_aS2eK4"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LT9AU6jghhch"},"id":"LT9AU6jghhch","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","path_files ='/content/drive/MyDrive/Colab Notebooks/Files'"],"metadata":{"id":"tRFF14OYh2Ws"},"id":"tRFF14OYh2Ws","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***OR***"],"metadata":{"id":"Zvg8Shzb6G4H"},"id":"Zvg8Shzb6G4H"},{"cell_type":"markdown","source":["# Connection path definition (Just for AWS Jupiter Notebook)"],"metadata":{"id":"PAg9Kvp-6PL1"},"id":"PAg9Kvp-6PL1"},{"cell_type":"code","source":["import sys\n","path_files ='./Files'"],"metadata":{"id":"k3ODMac16VEc"},"id":"k3ODMac16VEc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import libraries"],"metadata":{"id":"OVqOBg_g2iZc"},"id":"OVqOBg_g2iZc"},{"cell_type":"code","source":["pip install seaborn"],"metadata":{"id":"foe0cseS-h1v"},"id":"foe0cseS-h1v","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install scikit-learn"],"metadata":{"id":"WHb1g6qu-n23"},"id":"WHb1g6qu-n23","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"tlT3wZv0-fui"},"id":"tlT3wZv0-fui"},{"cell_type":"code","execution_count":null,"id":"5bcc0754","metadata":{"id":"5bcc0754"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","\n","from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from datetime import datetime, timedelta\n","from os.path import join\n"]},{"cell_type":"markdown","source":["# [1] Data Collection (Filtered)"],"metadata":{"id":"uHndIvGM2mWO"},"id":"uHndIvGM2mWO"},{"cell_type":"markdown","source":["Read DataSet for processing\n","\n","\n","\n"],"metadata":{"id":"0bV_vNVg2wEk"},"id":"0bV_vNVg2wEk"},{"cell_type":"code","source":["new_metric_df=pd.read_csv(join(path_files,'bng_subscribers_new_metric.csv'),index_col=0)"],"metadata":{"id":"tJuX3ExX9KwV"},"id":"tJuX3ExX9KwV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_metric_df"],"metadata":{"id":"kZ9EZeUs9RJo"},"id":"kZ9EZeUs9RJo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory Data Analysis (EDA)"],"metadata":{"id":"H8plcFaLVZij"},"id":"H8plcFaLVZij"},{"cell_type":"markdown","source":["Joint Histogram: CPU vs Memory"],"metadata":{"id":"2egQi5NwPArU"},"id":"2egQi5NwPArU"},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(12,8)})\n","sns.histplot(data=new_metric_df,x='cpu_utilization', y='mem_utilization', bins=50)\n","plt.title('CPU and memory utilization in SD-WAN devices')\n","plt.ylabel('% Mem utilization')"],"metadata":{"id":"mCoK0E2-USp_"},"id":"mCoK0E2-USp_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing for Clustering Analysis"],"metadata":{"id":"rsZu1QY4VqyX"},"id":"rsZu1QY4VqyX"},{"cell_type":"markdown","source":["Select features"],"metadata":{"id":"VPw9OIxyPOtc"},"id":"VPw9OIxyPOtc"},{"cell_type":"code","source":["X = new_metric_df[['mem_utilization','cpu_utilization']].values\n"],"metadata":{"id":"CF4yzc4nb778"},"id":"CF4yzc4nb778","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Method 1: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"],"metadata":{"id":"pP2q80OS29At"},"id":"pP2q80OS29At"},{"cell_type":"markdown","source":["**Approach 1:** It is a density-based clustering algorithm that identifies groups of points in a dataset without the need to specify the number of clusters in advance. DBSCAN is useful when we do not know how many clusters there are and want to detect structures based on data density, as well as identify outliers."],"metadata":{"id":"VAp37emXuqmj"},"id":"VAp37emXuqmj"},{"cell_type":"markdown","source":["\n","\n","# [3] Data Normalization\n","\n","\n"],"metadata":{"id":"jUtViJcZPTQz"},"id":"jUtViJcZPTQz"},{"cell_type":"code","source":["scaler = StandardScaler()\n","scaler = scaler.fit(X)\n","X_scaled=scaler.transform(X)"],"metadata":{"id":"v3UsgHktuKVV"},"id":"v3UsgHktuKVV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [4] Apply Clustering method (DBSCAN)\n","\n","\n","\n","Compute nearest neighbors"],"metadata":{"id":"GB6VCESTP2Ax"},"id":"GB6VCESTP2Ax"},{"cell_type":"code","source":["neigh = NearestNeighbors(n_neighbors=2)\n","nbrs = neigh.fit(X_scaled)\n","distances, indices = nbrs.kneighbors(X_scaled)"],"metadata":{"id":"hfak3m08iEtf"},"id":"hfak3m08iEtf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting K-distance Graph\n","distances = np.sort(distances, axis=0)\n","distances = distances[:,1]\n","plt.figure(figsize=(20,10))\n","plt.plot(distances)\n","plt.title('K-distance Graph',fontsize=20)\n","plt.xlabel('Data Points sorted by distance',fontsize=14)\n","plt.ylabel('Epsilon',fontsize=14)\n","plt.show()"],"metadata":{"id":"WpOzk9DwiVGe"},"id":"WpOzk9DwiVGe","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defines two key parameters:  \n","   - **ε (epsilon):** The maximum distance within which points are considered to be in the same neighborhood.  \n","   - **MinPts (minimum points):** The minimum number of points required in a neighborhood to be considered a \"core\" of a cluster."],"metadata":{"id":"C160nJfIvLzg"},"id":"C160nJfIvLzg"},{"cell_type":"markdown","source":["**What does this graph show?**\n","This is known as the K-distance graph, and it's used to help determine the optimal value for eps (epsilon) in the DBSCAN clustering algorithm.\n","\n","**What does the graph represent?**\n","X-axis: Indices of data points, sorted by increasing distance to their second nearest neighbor (since n_neighbors=2 was used).\n","\n","Y-axis: Distance to the second nearest neighbor.\n","\n","**What are we looking for?**\n","The \"elbow\" of the curve — the point where the slope changes sharply — is a good estimate for eps.\n","\n","Before the elbow: Points are in dense regions (clusters), so their nearest neighbor distances are small.\n","\n"],"metadata":{"id":"7x-3USjsRPmO"},"id":"7x-3USjsRPmO"},{"cell_type":"markdown","source":["Run DBSCAN with estimated eps"],"metadata":{"id":"_voHxMSXRzV2"},"id":"_voHxMSXRzV2"},{"cell_type":"code","source":["dbscan=DBSCAN(eps=0.8,min_samples=10)\n","dbscan.fit(X_scaled)"],"metadata":{"id":"6cxDIkPsfefT"},"id":"6cxDIkPsfefT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add labels to DataFrame"],"metadata":{"id":"157Zd_IER29V"},"id":"157Zd_IER29V"},{"cell_type":"code","source":["dbscan.labels_"],"metadata":{"id":"6hUz71yygiiH"},"id":"6hUz71yygiiH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [5] Visualize DBSCAN results"],"metadata":{"id":"gddYQg_GR6FJ"},"id":"gddYQg_GR6FJ"},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(12,8)})\n","sns.scatterplot(data=new_metric_df,x='cpu_utilization', y='mem_utilization',hue=dbscan.labels_)\n","plt.title('CPU and memory utilization clusters with DBSCAN in SD-WAN devices')"],"metadata":{"id":"YMexK5dpfsoH"},"id":"YMexK5dpfsoH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Advantages of DBSCAN:  \n","✅ Does not require defining the number of clusters (K) in advance.  \n","✅ Automatically identifies outliers.  \n","✅ Works well with arbitrarily shaped and differently sized clusters."],"metadata":{"id":"75sJjgorvp_e"},"id":"75sJjgorvp_e"},{"cell_type":"markdown","source":["Disadvantages of DBSCAN:  \n","❌ Sensitive to the choice of **ε** and **MinPts**.  \n","❌ Does not perform well with high-dimensional datasets.  \n","❌ Struggles with clusters that have highly variable densities."],"metadata":{"id":"z-p7cr_Dvi8t"},"id":"z-p7cr_Dvi8t"},{"cell_type":"markdown","source":["# Method 2: K-Means"],"metadata":{"id":"QftWuDrW3B1K"},"id":"QftWuDrW3B1K"},{"cell_type":"markdown","source":["**Approach 2:** K-Means is a clustering algorithm that groups data into K clusters based on the similarity of their features. It is one of the most popular and efficient algorithms for data segmentation."],"metadata":{"id":"8uCegHk6wJCw"},"id":"8uCegHk6wJCw"},{"cell_type":"markdown","source":["In machine learning, the common approach is brute force: repeating the process multiple times to find the best result. The Elbow method follows this principle by running K-means for different values of K to determine the optimal number of clusters. We iterate over various K values, executing the K-means model in each iteration with the `n_clusters` variable (corresponding to K) and storing the WSS results for each iteration."],"metadata":{"id":"iQ1f9bMdwayh"},"id":"iQ1f9bMdwayh"},{"cell_type":"markdown","source":["### **Elbow Method for Choosing K**  \n","To determine the optimal number of clusters, K-Means is run with different K values, measuring the **Within-Cluster Sum of Squares (WSS)**. Then, a plot of **K vs. WSS** is created, and the optimal K is identified where the WSS reduction slows down significantly."],"metadata":{"id":"XshLOtaQw1er"},"id":"XshLOtaQw1er"},{"cell_type":"markdown","source":["Elbow method to determine optimal K"],"metadata":{"id":"heb_c0WTSODm"},"id":"heb_c0WTSODm"},{"cell_type":"markdown","source":["# [4] Apply Clustering method (K-Means)\n","\n","\n","\n"],"metadata":{"id":"2pVqXYehscoZ"},"id":"2pVqXYehscoZ"},{"cell_type":"code","source":["wss = []\n","for i in range(1,11):\n","  kmeans = KMeans(n_clusters=i,init=\"k-means++\",random_state=42)\n","  kmeans.fit(X_scaled)\n","  wss.append(kmeans.inertia_)\n"],"metadata":{"id":"pEuGC63HX98L"},"id":"pEuGC63HX98L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(12,8)})\n","sns.lineplot(x = range(1,11), y = wss)\n","plt.ylabel('WSS')\n","plt.xlabel('K')\n","plt.title('Elbow method to select K number of clusters')"],"metadata":{"id":"F-Pmw4x2Yl2U"},"id":"F-Pmw4x2Yl2U","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply KMeans with chosen K"],"metadata":{"id":"KT4MZLXoSZUG"},"id":"KT4MZLXoSZUG"},{"cell_type":"markdown","source":["Example with K=3 >>>"],"metadata":{"id":"6jldaNBvwiNV"},"id":"6jldaNBvwiNV"},{"cell_type":"code","source":["K = 3"],"metadata":{"id":"3_aDN1m_Z2mX"},"id":"3_aDN1m_Z2mX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["KMeans_model = KMeans(n_clusters=K,init=\"k-means++\",random_state=42)\n","KMeans_clusters = KMeans_model.fit_predict(X_scaled)\n","new_metric_df['cluster']=KMeans_clusters"],"metadata":{"id":"FFzYWjGhZ4lO"},"id":"FFzYWjGhZ4lO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [5] Visualize K-Means results\n","\n"],"metadata":{"id":"GCw5EYvtsoUc"},"id":"GCw5EYvtsoUc"},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(12,8)})\n","sns.scatterplot(data=new_metric_df,x='cpu_utilization', y='mem_utilization',hue='cluster')\n","plt.title('CPU and memory utilization clusters with K-means in SD-WAN devices')"],"metadata":{"id":"TnpZE5VdaXrf"},"id":"TnpZE5VdaXrf","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **Advantages of K-Means**  \n","✅ Fast and scalable for large datasets.  \n","✅ Works well when clusters have spherical shapes and similar sizes.  \n","✅ Easy to implement and interpret.  \n","\n"," **Disadvantages of K-Means**  \n","❌ Requires specifying K in advance (though the Elbow Method helps choose it).  \n","❌ Struggles with irregularly shaped clusters or varying densities.  \n","❌ Sensitive to outliers and the choice of initial centroids."],"metadata":{"id":"hvkmDHPSxQje"},"id":"hvkmDHPSxQje"},{"cell_type":"markdown","source":["# Method 3: Hierarchical Clustering"],"metadata":{"id":"23AjJHbW3Ga9"},"id":"23AjJHbW3Ga9"},{"cell_type":"markdown","source":["**Approach 3:** Hierarchical clustering is a clustering method that organizes data into a hierarchical structure, in the form of a dendrogram (a clustering tree). Unlike K-Means, it does not require defining the number of clusters in advance and allows visualizing the relationships between the data."],"metadata":{"id":"6XW5IIauyDSF"},"id":"6XW5IIauyDSF"},{"cell_type":"markdown","source":["Compute linkage matrix"],"metadata":{"id":"K-gtSG9NSw31"},"id":"K-gtSG9NSw31"},{"cell_type":"code","source":["Z=linkage(X,'ward')\n","\n","# Plotting Dendogram\n","dendrogram(Z, leaf_rotation=45., leaf_font_size=12.)\n","plt.title('Hierarchical Clustering Dendrogram')\n","plt.xlabel('Cluster Size')\n","plt.ylabel('Distance')\n","plt.rcParams[\"figure.figsize\"] = (14,6)\n","plt.show()"],"metadata":{"id":"jztj6_JFctlF"},"id":"jztj6_JFctlF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This text explains how to interpret a **dendrogram** (a graphical representation used in hierarchical clustering) to determine the number of clusters in a dataset. Each horizontal line represents a cluster, and its position on the **y-axis** (distance) shows the separation between clusters. The higher the value on the **y-axis**, the greater the distance between clusters, indicating that they are likely to be different from each other.\n","\n","The goal is to find a \"cut-off point\" in the graph— a horizontal distance at which clusters should be separated. In this case, the cut-off is between values **100 and 150**, resulting in **3 clusters** since three vertical lines intersect the horizontal line. After determining the cut-off, the **Hierarchical Agglomerative Clustering** algorithm can be applied to label data points with their corresponding cluster labels. The result (3 clusters) is consistent with what was obtained using **K-means**, confirming the validity of the results.\n","\n"],"metadata":{"id":"j7sBLYDv0YOs"},"id":"j7sBLYDv0YOs"},{"cell_type":"code","source":["K=3"],"metadata":{"id":"fP-0LtD-dss7"},"id":"fP-0LtD-dss7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_hcluster = AgglomerativeClustering(n_clusters=K,linkage='ward')\n","model_hcluster.fit(X)"],"metadata":{"id":"ZRZe-HFOduwc"},"id":"ZRZe-HFOduwc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_metric_df['hcluster']=model_hcluster.labels_"],"metadata":{"id":"dMJl1RP4eFjc"},"id":"dMJl1RP4eFjc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [5] Visualize Hirarchical Clustering results"],"metadata":{"id":"2thamUZGtUmW"},"id":"2thamUZGtUmW"},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(12,8)})\n","sns.scatterplot(data=new_metric_df,x='cpu_utilization', y='mem_utilization',hue='hcluster')\n","plt.title('CPU and memory utilization clusters with Agglomerative Hierarchical Clustering in SD-WAN devices')"],"metadata":{"id":"AtSzMkFseWD9"},"id":"AtSzMkFseWD9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Advantages of Hierarchical Clustering**  \n","✅ Does not require defining the number of clusters (K) in advance.  \n","✅ Generates a dendrogram that helps visualize the relationships between the data.  \n","✅ Works well when clusters have natural hierarchical structures.  \n","\n","### **Disadvantages of Hierarchical Clustering**  \n","❌ Computationally expensive for large datasets.  \n","❌ Does not handle outliers well.  \n","❌ Cannot be \"re-adjusted\" once clustering is done (unlike K-Means)."],"metadata":{"id":"EaUN2-El02AR"},"id":"EaUN2-El02AR"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"14rtqZTGy7NUBBCzByhdtKYxHMKklReGj","timestamp":1668192517265},{"file_id":"1NXeAHZ_8xd0_KopyXaAnkpQMi0Sx54Qs","timestamp":1668179877689},{"file_id":"17b-86MhtHHRjH5dny85MflJdlgFhDkG_","timestamp":1668104079000}]}},"nbformat":4,"nbformat_minor":5}