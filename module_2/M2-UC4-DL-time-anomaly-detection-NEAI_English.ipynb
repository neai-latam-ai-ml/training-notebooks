{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wOKrwVlmP37w",
   "metadata": {
    "id": "wOKrwVlmP37w"
   },
   "source": [
    "# **UC4. Is there something wrong with my network subscribers? DL Approach**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZNGYcRKChT1O",
   "metadata": {
    "id": "ZNGYcRKChT1O"
   },
   "source": [
    "In our network, the Provider Edge devices also function as Broadband Network Gateways (BNGs), providing Internet access to subscribers based on their region\n",
    "\n",
    "The challenge is to determine whether the number of subscribers in a BNG is normal or not. Since this metric changes over time due to seasonality, setting a fixed threshold is not feasible. Instead, a mechanism is needed to learn the normal values and detect deviations as anomalies.\n",
    "\n",
    "The goal is not to optimize these algorithms but to demonstrate how they can be programmed, trained, and how they perform in this use case. The same methodology can be applied to other metrics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X-84_vIL0S2P",
   "metadata": {
    "id": "X-84_vIL0S2P"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow==2.19.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jzg9JQj70F7X",
   "metadata": {
    "id": "Jzg9JQj70F7X"
   },
   "source": [
    "# Extra libraries to install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TVl-jYF_0MK8",
   "metadata": {
    "id": "TVl-jYF_0MK8"
   },
   "source": [
    "# Connection to drive and path definition (Just for Google Colab Lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LT9AU6jghhch",
   "metadata": {
    "id": "LT9AU6jghhch"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tRFF14OYh2Ws",
   "metadata": {
    "id": "tRFF14OYh2Ws"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "path_files =('/content/drive/MyDrive/Colab Notebooks/Files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7qXussfoUyi-",
   "metadata": {
    "id": "7qXussfoUyi-"
   },
   "source": [
    "# ***OR***\n",
    "\n",
    "# Connection path definition (Just for AWS Jupiter Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PF3WnkjmU9QT",
   "metadata": {
    "id": "PF3WnkjmU9QT"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "path_files ='./Files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WPnvTAUz0roT",
   "metadata": {
    "id": "WPnvTAUz0roT"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc0754",
   "metadata": {
    "id": "5bcc0754"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
    "import time\n",
    "from calendar import timegm, monthrange\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, Dropout, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy,Accuracy,BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UbNqkorW6rjQ",
   "metadata": {
    "id": "UbNqkorW6rjQ"
   },
   "source": [
    "# [1] Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nZMFdz0n6u6c",
   "metadata": {
    "id": "nZMFdz0n6u6c"
   },
   "source": [
    " We will load a data set that contains number of subscribers for PE-4 for a period of 30 days, with **5 minute frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4iu-zhHcJuib",
   "metadata": {
    "id": "4iu-zhHcJuib"
   },
   "outputs": [],
   "source": [
    "metric_df=pd.read_csv(join(path_files,'bng_subscribers_metric.csv'),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b6b01",
   "metadata": {
    "id": "819b6b01"
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69794621",
   "metadata": {
    "id": "69794621"
   },
   "outputs": [],
   "source": [
    "ds_column = []\n",
    "for i,obs in metric_df.iterrows():\n",
    "    #new_timestamp = datetime.fromtimestamp(obs['timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    new_timestamp = datetime.fromtimestamp(obs['timestamp'])\n",
    "    ds_column.append(new_timestamp)\n",
    "metric_df['ds']=ds_column\n",
    "metric_df['y']=metric_df['bng_subscribers']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ac4cb",
   "metadata": {
    "id": "b90ac4cb"
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mCv3lyOo7LMf",
   "metadata": {
    "id": "mCv3lyOo7LMf"
   },
   "source": [
    "This is how the data looks like for the target metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50ff81",
   "metadata": {
    "id": "fd50ff81"
   },
   "outputs": [],
   "source": [
    "# Visualize data using seaborn\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.lineplot(x=metric_df['ds'][:2000], y=metric_df['bng_subscribers'][:2000])\n",
    "plt.title('BNG Subscribers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SyJBstzC7ggz",
   "metadata": {
    "id": "SyJBstzC7ggz"
   },
   "source": [
    "# LSTM based autoencoder (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kIEyq7n77kCd",
   "metadata": {
    "id": "kIEyq7n77kCd"
   },
   "source": [
    "**Approach 4:** We will start exploring the possibility to detect anomalies using Neural Networks and deep learning. In particular in this case, we will start with using a sequence based autoencoder. The sequence based aspect results in the approach of using LSTM. LSTM is well built for sequences, so the intuition says that it may be a good choice. However, it has been reported that LSTM does not work specially well with auto-regresive signals, so we will need to check how well in performs in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HVwRSup1g1k5",
   "metadata": {
    "id": "HVwRSup1g1k5"
   },
   "source": [
    "We will use 95% for training and 5% for testing or validation. The reason we use a larger percentage for training is that, in this particular test, we do not have many observations, only around 8,600."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCNtsvwcEr4u",
   "metadata": {
    "id": "cCNtsvwcEr4u"
   },
   "source": [
    "Data Nomalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b3a03",
   "metadata": {
    "id": "a01b3a03"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(metric_df[['y']])\n",
    "metric_df['y_scaled']=scaler.transform(metric_df[['y']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2MSYWWqHpAi9",
   "metadata": {
    "id": "2MSYWWqHpAi9"
   },
   "source": [
    "# [3] DataSet Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8798bfb",
   "metadata": {
    "id": "e8798bfb"
   },
   "outputs": [],
   "source": [
    "train_pct = 0.95\n",
    "train_size = int(metric_df.shape[0]*train_pct)\n",
    "test_size = metric_df.shape[0]-train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HpLIofcfpH-_",
   "metadata": {
    "id": "HpLIofcfpH-_"
   },
   "source": [
    "# [4] Formatting for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_C0-WclOmeAt",
   "metadata": {
    "id": "_C0-WclOmeAt"
   },
   "outputs": [],
   "source": [
    "time_steps = 72\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcWBTmOrE95c",
   "metadata": {
    "id": "dcWBTmOrE95c"
   },
   "source": [
    "Create sequences of Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5ef6e",
   "metadata": {
    "id": "7eb5ef6e"
   },
   "outputs": [],
   "source": [
    "Xs, ys = [], []\n",
    "for i in range(metric_df.shape[0] - time_steps):\n",
    "    v = metric_df[['y_scaled']].iloc[i:(i + time_steps)].values\n",
    "    Xs.append(v)\n",
    "    ys.append(metric_df.y_scaled.iloc[i + time_steps-1])\n",
    "x = np.array(Xs)\n",
    "y = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GnEh8ad6rrDd",
   "metadata": {
    "id": "GnEh8ad6rrDd"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ys5Vs16FLrb",
   "metadata": {
    "id": "7ys5Vs16FLrb"
   },
   "source": [
    "# [5] Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f3cbc",
   "metadata": {
    "id": "775f3cbc"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=128,input_shape=(x_train.shape[1],x_train.shape[2]),return_sequences=False))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(RepeatVector(n=x_train.shape[1]))\n",
    "model.add(LSTM(units=128,return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(keras.layers.Dense(units=x_train.shape[2])))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.compile(loss='mse',optimizer =Adam(),metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fDfjNWrm2gB",
   "metadata": {
    "id": "6fDfjNWrm2gB"
   },
   "source": [
    "The model compilation command defines how training will be conducted. The \"compile\" method specifies the loss function (MSE in this case), which depends on the use case. The optimizer (here, \"adam\") controls weight updates during training. Additionally, selected metrics help monitor and guide the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jih4gHLtuu7n",
   "metadata": {
    "id": "Jih4gHLtuu7n"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q-py5uXGFX0f",
   "metadata": {
    "id": "q-py5uXGFX0f"
   },
   "source": [
    "# [6] Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09Ef3oHytP2B",
   "metadata": {
    "id": "09Ef3oHytP2B"
   },
   "outputs": [],
   "source": [
    "filepath=join(path_files,\"weights_autoencoder_auto_mse_best.weights.h5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='mse', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2nlEe1P0uYo4",
   "metadata": {
    "id": "2nlEe1P0uYo4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "r27ocMhKnazP",
   "metadata": {
    "id": "r27ocMhKnazP"
   },
   "source": [
    "A **checkpoint** ensures model weights are saved during training to prevent data loss if the process is interrupted (e.g., crashes, disconnections). It defines:  \n",
    "- **Metric to monitor** (MSE).  \n",
    "- **When to save weights** (**save_best_only**).  \n",
    "- **Condition to save** (if the metric improves).  \n",
    "- **File to store weights**.  \n",
    "\n",
    "This allows resuming training with the last best weights, maintaining progress even if the process is interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZDeLoXqcoHcR",
   "metadata": {
    "id": "ZDeLoXqcoHcR"
   },
   "source": [
    "**Model Training:** This command initiates the training process. The first two parameters are typically the training data (X, y), but for an autoencoder, we use **X, X** so the label is the same as the input data.  \n",
    "\n",
    "An **epoch** is one full iteration over the dataset, and the number of epochs needed depends on the use case. You can set **callbacks** to stop training if the model's performance doesn't improve by a certain percentage. In this case, we’ve set 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QX5wBbNgp4n0",
   "metadata": {
    "id": "QX5wBbNgp4n0"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4PzNhJqtufJI",
   "metadata": {
    "id": "4PzNhJqtufJI"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train,x_train, epochs =50 , batch_size=128,validation_split=0.1,shuffle = False,callbacks=callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lTVxbDhCvLAI",
   "metadata": {
    "id": "lTVxbDhCvLAI"
   },
   "outputs": [],
   "source": [
    "model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BVqDODjUvsrD",
   "metadata": {
    "id": "BVqDODjUvsrD"
   },
   "outputs": [],
   "source": [
    "model.save(join(path_files,'anomaly_model_autoencoder_best.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vCjiLr05Dduf",
   "metadata": {
    "id": "vCjiLr05Dduf"
   },
   "source": [
    "# [7] Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bFEqNbXe4xM1",
   "metadata": {
    "id": "bFEqNbXe4xM1"
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k2_C6n7WhdkA",
   "metadata": {
    "id": "k2_C6n7WhdkA"
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltrc-78hhow9",
   "metadata": {
    "id": "ltrc-78hhow9"
   },
   "outputs": [],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0LpZWdcMDwlh",
   "metadata": {
    "id": "0LpZWdcMDwlh"
   },
   "source": [
    "## [8] Anomaly Detection / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqntNptoC8i5",
   "metadata": {
    "id": "tqntNptoC8i5"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "y_pred = y_pred[:, -1, 0].reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "mae_loss = np.abs(y_pred - y)\n",
    "mae_threshold = np.percentile(mae_loss, 99)\n",
    "\n",
    "score_df = pd.DataFrame(index=metric_df[time_steps:].ds)\n",
    "score_df['loss'] = mae_loss\n",
    "score_df['threshold'] = mae_threshold\n",
    "score_df['anomaly'] = score_df.loss > score_df.threshold\n",
    "score_df['y'] = y\n",
    "score_df['y_pred'] = y_pred\n",
    "\n",
    "total_anomalies = score_df[score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YHxky57wD5fC",
   "metadata": {
    "id": "YHxky57wD5fC"
   },
   "source": [
    "# --- Plot global anomalies ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "el-QaLh_-aYn",
   "metadata": {
    "id": "el-QaLh_-aYn"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x= score_df.index, y= score_df.loss,label='loss')\n",
    "sns.lineplot(x = score_df.index, y =score_df.threshold,label='threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMc5Ox0--jOx",
   "metadata": {
    "id": "EMc5Ox0--jOx"
   },
   "outputs": [],
   "source": [
    "scope = 2000\n",
    "scope_time = score_df.index[scope]\n",
    "anomaly_scope = total_anomalies.index <= scope_time\n",
    "sns.lineplot(\n",
    "      x= score_df[:scope].index,\n",
    "      y= scaler.inverse_transform(score_df[:scope].y.values.reshape(1,-1)).reshape(-1),\n",
    "      label='actual bng_subscribers'\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "      x= score_df[:scope].index,\n",
    "      y= scaler.inverse_transform(score_df[:scope].y_pred.values.reshape(1,-1)).reshape(-1),\n",
    "      label='predicted bng_subscribers'\n",
    "    )\n",
    "\n",
    "sns.scatterplot(\n",
    "      x= total_anomalies[anomaly_scope].index,\n",
    "      y= scaler.inverse_transform(total_anomalies[anomaly_scope].y.values.reshape(1,-1)).reshape(-1),\n",
    "      color=sns.color_palette()[3],\n",
    "      s=52,\n",
    "      label='anomaly'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HfOCEm1QsKtl",
   "metadata": {
    "id": "HfOCEm1QsKtl"
   },
   "source": [
    "# Feed Forward Dense layers based Autoencoder (FNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1JW-YUaIxDK",
   "metadata": {
    "id": "p1JW-YUaIxDK"
   },
   "source": [
    "\n",
    "Create sequences of size 'time_steps' for the input and the label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dWlrUd4Inpo",
   "metadata": {
    "id": "6dWlrUd4Inpo"
   },
   "source": [
    "# [3] Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oyRLl1AythGe",
   "metadata": {
    "id": "oyRLl1AythGe"
   },
   "outputs": [],
   "source": [
    "time_steps = 2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_VzFmC2-trJd",
   "metadata": {
    "id": "_VzFmC2-trJd"
   },
   "outputs": [],
   "source": [
    "Xs, ys = [], []\n",
    "for i in range(metric_df.shape[0] - time_steps):\n",
    "    v = metric_df[['y_scaled']].iloc[i:(i + time_steps)].values\n",
    "    Xs.append(v)\n",
    "    ys.append(metric_df.y_scaled.iloc[i + time_steps-1])\n",
    "x = np.array(Xs)\n",
    "y = np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VpefQqshscn2",
   "metadata": {
    "id": "VpefQqshscn2"
   },
   "source": [
    "# [4] DataSet Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VvxB-HkPtrJe",
   "metadata": {
    "id": "VvxB-HkPtrJe"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V1MwpYL6I5eQ",
   "metadata": {
    "id": "V1MwpYL6I5eQ"
   },
   "source": [
    "# [5] Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vxbA2UpsQ1H",
   "metadata": {
    "id": "0vxbA2UpsQ1H"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(units=1024,input_shape=(x_train.shape[1],),activation='tanh'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(units=512,activation='tanh'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(units=128,activation='tanh'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(units=512,activation='tanh'))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(units=x_train.shape[1]))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VWeO4sbnI9jH",
   "metadata": {
    "id": "VWeO4sbnI9jH"
   },
   "source": [
    "Model compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5yh5MQQbI83L",
   "metadata": {
    "id": "5yh5MQQbI83L"
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(loss='mse',optimizer =opt,metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3GsvxVXJBkY",
   "metadata": {
    "id": "u3GsvxVXJBkY"
   },
   "source": [
    "Model brief:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8Jn7CAQtWZe",
   "metadata": {
    "id": "c8Jn7CAQtWZe"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MAc-9LNms3iK",
   "metadata": {
    "id": "MAc-9LNms3iK"
   },
   "source": [
    "A **checkpoint** ensures model weights are saved during training to prevent data loss if the process is interrupted (e.g., crashes, disconnections). It defines:  \n",
    "- **Metric to monitor** (MSE).  \n",
    "- **When to save weights** (**save_best_only**).  \n",
    "- **Condition to save** (if the metric improves).  \n",
    "- **File to store weights**.  \n",
    "\n",
    "This allows resuming training with the last best weights, maintaining progress even if the process is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GNjB6Dd9t5HE",
   "metadata": {
    "id": "GNjB6Dd9t5HE"
   },
   "outputs": [],
   "source": [
    "filepath=join(path_files,\"weights_autoencoder_dense_mse_best.weights.h5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='mse', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pu8RIVJusv_s",
   "metadata": {
    "id": "Pu8RIVJusv_s"
   },
   "source": [
    "**Model Training:** This command initiates the training process. The first two parameters are typically the training data (X, y), but for an autoencoder, we use **X, X** so the label is the same as the input data.  \n",
    "\n",
    "An **epoch** is one full iteration over the dataset, and the number of epochs needed depends on the use case. You can set **callbacks** to stop training if the model's performance doesn't improve by a certain percentage. In this case, we’ve set 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fzMSymv9JJnT",
   "metadata": {
    "id": "fzMSymv9JJnT"
   },
   "source": [
    "# [6] Model Training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XwHRkm7luB6W",
   "metadata": {
    "id": "XwHRkm7luB6W"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train,x_train, epochs =50 , batch_size=128,validation_split=0.1,shuffle = False,callbacks=callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbPu-EwYuVQc",
   "metadata": {
    "id": "sbPu-EwYuVQc"
   },
   "outputs": [],
   "source": [
    "model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CxgkEjv-JVNW",
   "metadata": {
    "id": "CxgkEjv-JVNW"
   },
   "source": [
    "# [7] Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NkUhp8b-uFpF",
   "metadata": {
    "id": "NkUhp8b-uFpF"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vridaeWlJYdJ",
   "metadata": {
    "id": "vridaeWlJYdJ"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "y_pred = y_pred[:, -1]  # Último valor reconstruido de cada secuencia\n",
    "y_pred = y_pred.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fwJVmIiuJiSo",
   "metadata": {
    "id": "fwJVmIiuJiSo"
   },
   "source": [
    "# Calcular error absoluto (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KpSJHEFeJgH2",
   "metadata": {
    "id": "KpSJHEFeJgH2"
   },
   "outputs": [],
   "source": [
    "mae_loss = np.abs(y_pred - y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDSOoq0iJoWj",
   "metadata": {
    "id": "tDSOoq0iJoWj"
   },
   "source": [
    "# Definir umbral para anomalías (percentil 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jeYfoQVJJpqk",
   "metadata": {
    "id": "jeYfoQVJJpqk"
   },
   "outputs": [],
   "source": [
    "mae_threshold = np.percentile(mae_loss, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H-lTVdZ0Jtqs",
   "metadata": {
    "id": "H-lTVdZ0Jtqs"
   },
   "source": [
    "# Crear DataFrame con resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-suJSZuu2au",
   "metadata": {
    "id": "0-suJSZuu2au"
   },
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(index=metric_df[(time_steps):].ds)\n",
    "score_df['loss'] = mae_loss\n",
    "score_df['threshold'] = mae_threshold\n",
    "score_df['anomaly'] = score_df.loss > score_df.threshold\n",
    "score_df['y'] = y\n",
    "score_df['y_pred']=y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WtzKrZhbJ1cF",
   "metadata": {
    "id": "WtzKrZhbJ1cF"
   },
   "source": [
    "# Filtrar anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7PPLJ5bCJ03V",
   "metadata": {
    "id": "7PPLJ5bCJ03V"
   },
   "outputs": [],
   "source": [
    "\n",
    "total_anomalies = score_df[score_df.anomaly == True]\n",
    "total_anomalies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_tik_EPOu2au",
   "metadata": {
    "id": "_tik_EPOu2au"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x= score_df[:scope].index, y= score_df[:scope].loss,label='loss')\n",
    "sns.lineplot(x = score_df[:scope].index, y =score_df[:scope].threshold,label='threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ETVNWGRtK7s",
   "metadata": {
    "id": "1ETVNWGRtK7s"
   },
   "source": [
    "In the previous visualization, we are capturing the \"difference\" between the predicted value and the actual value of the metric. Effectively, when the loss is high, it means the metric is not behaving as the model has \"learned,\" so it can be considered an anomaly. We have calculated a threshold as the 99th percentile of the loss values, so we consider that, beyond this point, a value will be determined as anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lgg4WMMcJ7IA",
   "metadata": {
    "id": "Lgg4WMMcJ7IA"
   },
   "source": [
    "# --- Visualización de resultados ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twKiRxRLu2au",
   "metadata": {
    "id": "twKiRxRLu2au"
   },
   "outputs": [],
   "source": [
    "scope = 2000\n",
    "scope_time = score_df.index[scope]\n",
    "anomaly_scope = total_anomalies.index <= scope_time\n",
    "sns.lineplot(\n",
    "      x= score_df[:scope].index,\n",
    "      y= scaler.inverse_transform(score_df[:scope].y.values.reshape(1,-1)).reshape(-1),\n",
    "      label='actual bng_subscribers'\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "      x= score_df[:scope].index,\n",
    "      y= scaler.inverse_transform(score_df[:scope].y_pred.values.reshape(1,-1)).reshape(-1),\n",
    "      label='predicted bng_subscribers'\n",
    "    )\n",
    "\n",
    "sns.scatterplot(\n",
    "      x= total_anomalies[anomaly_scope].index,\n",
    "      y= scaler.inverse_transform(total_anomalies[anomaly_scope].y.values.reshape(1,-1)).reshape(-1),\n",
    "      color=sns.color_palette()[3],\n",
    "      s=52,\n",
    "      label='anomaly'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1dJFyR_8USaIJNRno4za-yHF3MZM1F38M",
     "timestamp": 1747237118380
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
