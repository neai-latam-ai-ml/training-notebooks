{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435e5799",
   "metadata": {},
   "source": [
    "## **Install required packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574952d-e6be-4171-8406-706962536213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install progressbar\n",
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21417c",
   "metadata": {},
   "source": [
    "## **Import requiered packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34375d6-eb3e-432e-af12-ae18bc901fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "import gc\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26450aaf",
   "metadata": {},
   "source": [
    "## **Defined constants**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778b1c7-fca0-4473-95cc-b07af6983cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "#configuration_template_file = 'config_template.txt'\n",
    "configuration_start_point = '!'\n",
    "test_samples = 10\n",
    "context_before = 50\n",
    "context_after = 50\n",
    "max_dictionary_size = 50000\n",
    "top_n_probabilities = 10\n",
    "probability_threshold = 3\n",
    "word_variability_threshold = 10\n",
    "latent_len = 512\n",
    "training_size = 100000\n",
    "testing_size = 2000\n",
    "total_subset = training_size+testing_size\n",
    "batch_size = 1024\n",
    "nb_epoch = 10\n",
    "training_steps_per_epoch = int(np.ceil(training_size/batch_size))\n",
    "validation_steps_per_epoch = int(np.ceil(testing_size/batch_size))\n",
    "test_file = 'config_test.cfg'\n",
    "tokenizer_file = 'tokenizer-pe.json'\n",
    "model_weights_file = 'weights_config_anomaly_PE_blstm.weights.h5'\n",
    "path_config_files = './configs'\n",
    "path_files = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21cc18",
   "metadata": {},
   "source": [
    "## **Define functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e37ac-7641-4c95-bd8b-3d87480b9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def config_train_test_split(target_configs, test_samples):\n",
    "  test_files_indexes = random.sample(range(len(target_configs)),test_samples)\n",
    "  train_configs = []\n",
    "  test_configs = []\n",
    "  for i in range(len(target_configs)):\n",
    "    if i in test_files_indexes:\n",
    "      test_configs.append(target_configs[i])\n",
    "    else:\n",
    "      train_configs.append(target_configs[i])\n",
    "  return train_configs,test_configs\n",
    "\n",
    "def find_config_start(config_lines, start_sequence):\n",
    "  i=0\n",
    "  config_found = np.False_\n",
    "  while not config_found:\n",
    "    if start_sequence in config_lines[i]:\n",
    "      config_found = True\n",
    "      config_index = i+1\n",
    "    else:\n",
    "      i+=1\n",
    "  return config_index\n",
    "\n",
    "def get_lines(config_file):\n",
    "  file_handle = open(config_file, 'r')\n",
    "  config_lines = file_handle.readlines()\n",
    "  return config_lines\n",
    "\n",
    "def get_config_lines(config_file):\n",
    "  file_handle = open(config_file, 'r')\n",
    "  config_lines = file_handle.readlines()\n",
    "  config_index = find_config_start(config_lines,configuration_start_point)\n",
    "  return config_lines[config_index:]\n",
    "\n",
    "def build_corpus(train_configs,path):\n",
    "  for config_file in train_configs:\n",
    "    corpus = []\n",
    "    print('Config:',config_file)\n",
    "    config_lines = get_config_lines(join(path,config_file))\n",
    "    for i in range(len(config_lines)):\n",
    "      corpus += config_lines[i].lower().split()\n",
    "    yield corpus\n",
    "\n",
    "def build_tokenizer(train_configs, path):\n",
    "  tokenizer = Tokenizer(filters='',oov_token='OOV',num_words=max_dictionary_size)\n",
    "  tokenizer.fit_on_texts(build_corpus(train_configs,path))\n",
    "  total_words = len(tokenizer.word_index) + 1\n",
    "  print(total_words)\n",
    "  return tokenizer\n",
    "\n",
    "def build_training_data(train_config, tokenizer):\n",
    "  input_sequences = []\n",
    "  input_labels = []\n",
    "  current_index = 0\n",
    "  corpus = []\n",
    "  config_lines = get_config_lines(train_config)\n",
    "  for i in range(len(config_lines)):\n",
    "      corpus += config_lines[i].lower().split()\n",
    "  while (current_index + context_before + context_after+1) < len(corpus):\n",
    "    current_sequence = corpus[current_index:(current_index+context_before)]+corpus[(current_index+context_before+1):(current_index+context_before+context_after+1)]\n",
    "    current_label = corpus[current_index+context_before]\n",
    "    token_list = tokenizer.texts_to_sequences([current_sequence])[0]\n",
    "    token_label = tokenizer.texts_to_sequences([current_label])[0]\n",
    "    input_sequences.append(token_list)\n",
    "    input_labels.append(token_label)\n",
    "    current_index +=1\n",
    "  xs, labels = np.array(input_sequences),np.array(input_labels)\n",
    "  ys = tf.keras.utils.to_categorical(labels, num_classes=(dictionary_size))\n",
    "  return xs,ys\n",
    "\n",
    "def build_training_data_batch(corpus, tokenizer, start_index, batch_size):\n",
    "  input_sequences = []\n",
    "  input_labels = []\n",
    "  current_index = start_index\n",
    "  config_finished=False\n",
    "  for i in range(batch_size):\n",
    "    if (current_index + context_before + context_after) < len(corpus):\n",
    "      current_sequence = corpus[current_index:(current_index+context_before)]+corpus[(current_index+context_before+1):(current_index+context_before+context_after+1)]\n",
    "      current_label = corpus[current_index+context_before]\n",
    "      token_list = tokenizer.texts_to_sequences([current_sequence])[0]\n",
    "      token_label = tokenizer.texts_to_sequences([current_label])[0]\n",
    "      input_sequences.append(token_list)\n",
    "      input_labels.append(token_label)\n",
    "      current_index +=1\n",
    "    else:\n",
    "      config_finished = True\n",
    "  xs, labels = np.array(input_sequences),np.array(input_labels)\n",
    "  ys = tf.keras.utils.to_categorical(labels, num_classes=(dictionary_size))\n",
    "  return xs,ys,config_finished\n",
    "\n",
    "def batch_generator(train_configs,path, batch_size, steps):\n",
    "  xs = []\n",
    "  ys = []\n",
    "  i=0\n",
    "  idx_total=1\n",
    "  while True:\n",
    "    config=train_configs[i]\n",
    "    corpus = []\n",
    "    config_lines=get_config_lines(join(path,config))\n",
    "    for j in range(len(config_lines)):\n",
    "      corpus += config_lines[j].lower().split()\n",
    "\n",
    "    idx=1\n",
    "    del xs\n",
    "    del ys\n",
    "    gc.collect()\n",
    "    done_yet = False\n",
    "    while not done_yet:\n",
    "      start_index = (idx-1)*batch_size\n",
    "      xs,ys,done_yet = build_training_data_batch(corpus,tokenizer,start_index,batch_size)\n",
    "      stop_index = start_index + batch_size\n",
    "      X_batch = xs\n",
    "      y_batch = ys\n",
    "      yield (X_batch,y_batch)\n",
    "      if not done_yet:\n",
    "        if idx_total < steps:\n",
    "            idx +=1\n",
    "            idx_total +=1\n",
    "        else:\n",
    "            idx_total=1\n",
    "            done_yet=True\n",
    "      else:\n",
    "        if idx < steps:\n",
    "            idx +=1\n",
    "            if i<(len(train_configs)-1):\n",
    "              i+=1\n",
    "            else:\n",
    "              i=0\n",
    "        else:\n",
    "            idx_total=1\n",
    "            i=0\n",
    "\n",
    "def predict_config_accuracy(config_file, model, tokenizer):\n",
    "  xs,ys = build_training_data(config_file, tokenizer)\n",
    "  score, acc = model.evaluate(xs[0:2000], ys[0:2000])\n",
    "  return score, acc\n",
    "\n",
    "def predict_config(config_file, model, tokenizer):\n",
    "  xs,ys = build_training_data(config_file, tokenizer)\n",
    "  y_pred = model(xs[0:1000])\n",
    "  return y_pred\n",
    "\n",
    "def convert_to_text(y_pred, tokenizer):\n",
    "  predicted_words_index = [int(np.argmax(y_pred[i,:])) for i in range(y_pred.shape[0])]\n",
    "  predicted_words = tokenizer.sequences_to_texts([predicted_words_index])[0]\n",
    "  return predicted_words\n",
    "\n",
    "def compute_config_anomaly_vector(config_file, model, tokenizer):\n",
    "  xs,ys = build_training_data(config_file, tokenizer)\n",
    "  #predicted_prob = model.predict_proba(xs, verbose=0)\n",
    "  y_pred = model(xs).numpy()\n",
    "  real_words_index = [int(np.argmax(ys[i,:])) for i in range(ys.shape[0])]\n",
    "  y_probability = []\n",
    "  y_recommended = []\n",
    "  y_prob_conf = []\n",
    "  for i in range(1,y_pred.shape[0]):\n",
    "    temp = np.partition(-y_pred[i,:], top_n_probabilities)\n",
    "    y_pred_average=np.mean(-temp[:top_n_probabilities])\n",
    "    y_pred_prob = y_pred[i,real_words_index[i]]\n",
    "    y_recom = np.argmax(y_pred[i,:])\n",
    "    word_variability = total_nexts[real_words_index[i-1]]\n",
    "    y_prob_conf = y_pred_prob/y_pred_average\n",
    "    if word_variability > word_variability_threshold:\n",
    "      probability = top_n_probabilities\n",
    "    else:\n",
    "      probability = y_pred_prob/y_pred_average\n",
    "    y_probability.append(probability)\n",
    "    y_recommended.append(y_recom)\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.hist(y_prob_conf, bins=30)\n",
    "  return y_probability,y_recommended\n",
    "\n",
    "def display_config_anomalies(config_file,model,tokenizer):\n",
    "  y_probability,y_recommended = compute_config_anomaly_vector(config_file,model,tokenizer)\n",
    "  config_lines = get_config_lines(config_file)\n",
    "  corpus = []\n",
    "  for i in range(len(config_lines)):\n",
    "      corpus += config_lines[i].lower().split()\n",
    "  y_words = corpus[context_before+1:-(context_after+1)]\n",
    "  probability_mask = np.array(y_probability) > probability_threshold\n",
    "  print('Number of anomalies detected:',(len(y_words)-np.sum(probability_mask)))\n",
    "  colored_config = ''\n",
    "  word_index = 0\n",
    "  for w in range(len(config_lines)):\n",
    "    tokenized_line = config_lines[w].split()\n",
    "    for item in tokenized_line:\n",
    "      if word_index>=(context_before+1) and word_index<(len(corpus)-(context_after+1)):\n",
    "        if probability_mask[word_index-(context_before+1)]:\n",
    "          colored_config += '\\033[92m '+item\n",
    "        else:\n",
    "          colored_config += '\\033[91m '+item + '(expected: '+tokenizer.sequences_to_texts([[y_recommended[word_index-(context_before+1)]]])[0]+')'\n",
    "      else:\n",
    "          colored_config += '\\033[94m '+item\n",
    "      word_index+=1\n",
    "    colored_config += '\\n'\n",
    "\n",
    "  print(colored_config)\n",
    "  return y_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097763f3",
   "metadata": {},
   "source": [
    "## **Load config files, divide the set (train/test), generate tokens**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db1759-8eec-4ac9-8f21-d2b64810bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files = [f for f in listdir(path_config_files) if (isfile(join(path_config_files, f)))]\n",
    "\n",
    "train_configs, test_configs = config_train_test_split(config_files, test_samples)\n",
    "\n",
    "#print('Training configs:',train_configs)\n",
    "#print('Testing configs:',test_configs)\n",
    "\n",
    "tokenizer = build_tokenizer(train_configs, path_config_files)\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8538359",
   "metadata": {},
   "source": [
    "## **Save tokens to file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402cb25a-2d05-42fa-985d-ef65ffef8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(\"./\", tokenizer_file), 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "with open(join(\"./\",tokenizer_file)) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaa856",
   "metadata": {},
   "source": [
    "## **Review discovered vocabulary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772f577-5798-47bf-bb2b-84011bd61952",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(tokenizer.word_index)+1\n",
    "dictionary_size= min(total_words,max_dictionary_size)\n",
    "print('Dictionary size:',dictionary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd67be",
   "metadata": {},
   "source": [
    "## **Get word contexts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946517bc-0fbd-4cbc-8bb7-74968c71ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_next_words = []\n",
    "for w in range(total_words):\n",
    "  total_next_words.append([])\n",
    "total_nexts = [0 for i in range(total_words)]\n",
    "pbar = ProgressBar()\n",
    "\n",
    "for config in pbar(train_configs):\n",
    "  corpus = []\n",
    "  config_lines = get_config_lines(join(path_config_files,config))\n",
    "  for i in range(len(config_lines)):\n",
    "    corpus += config_lines[i].lower().split()\n",
    "  for i in range(len(corpus)-1):\n",
    "    current_token = tokenizer.texts_to_sequences([corpus[i]])[0][0]\n",
    "    next_token = tokenizer.texts_to_sequences([corpus[i+1]])[0][0]\n",
    "    current_token_nexts = total_next_words[current_token]\n",
    "    if next_token not in current_token_nexts:\n",
    "      total_next_words[current_token].append(next_token)\n",
    "      total_nexts[current_token]+=1\n",
    "        \n",
    "my_training_batch_generator = batch_generator(train_configs, path_config_files, batch_size, training_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd708f2a",
   "metadata": {},
   "source": [
    "## **Define the model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cdc6e-7bea-4dfb-9c0f-bdda3a558a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mymodel = tf.keras.Sequential()\n",
    "mymodel.add(Embedding(dictionary_size, 100, input_length=(context_before+context_after)))\n",
    "mymodel.add(Bidirectional(LSTM(128,recurrent_dropout=0.1)))\n",
    "mymodel.add(Dense(dictionary_size,activation='softmax'))\n",
    "mymodel.add(layers.Dropout(0.1))\n",
    "input_shape_for_build = (None, context_before + context_after)\n",
    "mymodel.build(input_shape=input_shape_for_build)\n",
    "mymodel.summary()\n",
    "adam = Adam(learning_rate=0.01)\n",
    "mymodel.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c4f15",
   "metadata": {},
   "source": [
    "## **Save/Load model weights**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264226bc-f00b-4ea8-97ad-1fbfa1abc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.save_weights(join(path_files, model_weights_file))\n",
    "mymodel.load_weights(join(path_files, model_weights_file))\n",
    "\n",
    "filepath=join(path_files, model_weights_file)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93c02b",
   "metadata": {},
   "source": [
    "## **Start training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8eff-22a5-4e26-acfb-432b73d0e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mymodel.fit(my_training_batch_generator,\n",
    "                      epochs=nb_epoch,\n",
    "                      steps_per_epoch=training_steps_per_epoch,\n",
    "                      verbose=1,\n",
    "                      callbacks=callbacks_list\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653d830",
   "metadata": {},
   "source": [
    "## **Use model to review config-compliance deviation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcbdcd-5aab-446f-bf1f-0e6e9f7202f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = predict_config_accuracy(\"./config_PE-100.cfg\", mymodel, tokenizer)\n",
    "y_probability,_ = compute_config_anomaly_vector(\"./config_PE-100.cfg\",mymodel,tokenizer)\n",
    "y_words = display_config_anomalies(\"./config_PE-100.cfg\",mymodel,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7331326-200e-4131-b2b4-03020564932c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
