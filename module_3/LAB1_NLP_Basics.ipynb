{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install required libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install contractions\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import required libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH97tEIKanYn"
   },
   "source": [
    "## **Case Folding**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q7x74XvagoX"
   },
   "outputs": [],
   "source": [
    "txt = \"Segment routing works either on top of a MPLS network or on an IPv6 network.\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = txt.casefold()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw9e_OFGayO-"
   },
   "source": [
    "# **Special Character Removal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhoyTHXsa2Ej",
    "outputId": "5988b826-2321-4eb3-9981-4ee2aabc6039"
   },
   "outputs": [],
   "source": [
    "\n",
    "#input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#using regular expressions to remove spetial characters\n",
    "clear_str = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", input_str)\n",
    "\n",
    "print(clear_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhrqrFlKb07g",
    "outputId": "ec9d16ce-1111-445a-965f-b665b256b86f"
   },
   "outputs": [],
   "source": [
    "#Labraries in the field of NLP\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Function to clean the string\n",
    "def clean_text(text):\n",
    "  cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "  doc = nlp(cleaned_text)\n",
    "  return ' '.join(token.text for token in doc)\n",
    "\n",
    "# Get the final output\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb-hHuPivNFI",
    "outputId": "ea7feef4-5b32-4975-f062-c436a4eb374e"
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Tokenize\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "print(tokens)\n",
    "\n",
    "# Remove the special characters\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "clean_str = ' '.join(clean_tokens)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhv92mThwrK7"
   },
   "source": [
    "# **Handling contractions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q41mXNwIxX4U",
    "outputId": "ce987be8-ae1b-44f1-8272-fc25683a8bb3"
   },
   "outputs": [],
   "source": [
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "\n",
    "expanded_txt = contractions.fix(txt)\n",
    "\n",
    "print(expanded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgKpY45vyIaw",
    "outputId": "af0906d1-07d6-4a60-da03-ac566b82df7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "def expand_contractions(text):\n",
    "  contractions_pattern = {\n",
    "      r\"(?i)can't\": \"cannot\",\n",
    "      r\"(?i)won't\": \"will not\",\n",
    "      r\"(?i)it's\": \"it is\",\n",
    "      r\"(?i)weren't\": \"were not\",\n",
    "      r\"(?i)I'm\": \"I am\",\n",
    "      r\"(?i)haven't\": \"have not\"\n",
    "  }\n",
    "\n",
    "  for contractions, expansion in contractions_pattern.items():\n",
    "    text = re.sub(contractions, expansion, text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "expanded_text = expand_contractions(txt)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssIASpFzjMY"
   },
   "source": [
    "# **TOKENIZATION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhxfd0Fhzilp",
    "outputId": "408d7faf-6a8f-4b97-cc8f-059804447cc1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#sample text for tokenization\n",
    "txt = \"BGP used for routing within an autonomous system is called Interior Border Gateway Protocol (iBGP). In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(txt)\n",
    "print(len(words))\n",
    "print(words)\n",
    "\n",
    "# Sentence tokenization\n",
    "\n",
    "sent = sent_tokenize(txt)\n",
    "print(len(sent))\n",
    "print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m07VKPS-urZ"
   },
   "source": [
    "# **Stop words removal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz7HcJt-_BlU"
   },
   "outputs": [],
   "source": [
    "#sample sentence:\n",
    "sentence = \"This is a sample sentence, showing off the stop words filtration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dsfIVwf_LCQ",
    "outputId": "aec2789c-d5e3-433e-9b06-e99cc1d8c4c1"
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "nltk.download('stopwords')\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Filter stop words\n",
    "new_sentence = [word for word in words if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG9jajwDAClQ",
    "outputId": "9d60efcb-5858-4bd6-fe2b-77f6aea92fe0"
   },
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4yVHY6_77PC"
   },
   "source": [
    "#**N-Grams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itTXSz-E8E-5",
    "outputId": "b3655f54-e838-43cc-ab88-b47a62196a59"
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lmAhGg682OW",
    "outputId": "db309ddf-57d9-4875-93e4-7a123f96f716"
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "  tokens = word_tokenize(text)\n",
    "  n_gram = list(ngrams(tokens, n))\n",
    "  return n_gram\n",
    "\n",
    "txt = \"SRv6 is replacing MPLS\"\n",
    "\n",
    "unigrams = generate_ngrams(txt, 1)\n",
    "bigrams = generate_ngrams(txt, 2)\n",
    "trigrams = generate_ngrams(txt, 3)\n",
    "\n",
    "print(unigrams)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zH97tEIKanYn",
    "Iw9e_OFGayO-",
    "bhv92mThwrK7",
    "pssIASpFzjMY",
    "5m07VKPS-urZ",
    "d4yVHY6_77PC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
