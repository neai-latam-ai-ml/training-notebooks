{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH97tEIKanYn"
   },
   "source": [
    "# **Case Folding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlw2PnBtwkVM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q7x74XvagoX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw9e_OFGayO-"
   },
   "source": [
    "# **Special Character Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhoyTHXsa2Ej",
    "outputId": "5988b826-2321-4eb3-9981-4ee2aabc6039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#using regular expressions to remove spetial characters\n",
    "clear_str = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", input_str)\n",
    "\n",
    "print(clear_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m146.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhrqrFlKb07g",
    "outputId": "ec9d16ce-1111-445a-965f-b665b256b86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "#Labraries in the field of NLP\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Function to clean the string\n",
    "def clean_text(text):\n",
    "  cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "  doc = nlp(cleaned_text)\n",
    "  return ' '.join(token.text for token in doc)\n",
    "\n",
    "# Get the final output\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmWs3JZZvGOo"
   },
   "source": [
    "Other Library: nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/ubuntu/venv/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/venv/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb-hHuPivNFI",
    "outputId": "ea7feef4-5b32-4975-f062-c436a4eb374e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', '$', '$', '*', 'doing', '?']\n",
      "hello how are you doing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Tokenize\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "print(tokens)\n",
    "\n",
    "# Remove the special characters\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "clean_str = ' '.join(clean_tokens)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhv92mThwrK7"
   },
   "source": [
    "# **Handling contractions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiWug0dKwqZI",
    "outputId": "b1a2b7d4-f9a8-4242-c170-b65747fb7ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q41mXNwIxX4U",
    "outputId": "ce987be8-ae1b-44f1-8272-fc25683a8bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot go to walk it is raining. I have not found what I am looking for\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "\n",
    "expanded_txt = contractions.fix(txt)\n",
    "\n",
    "print(expanded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgKpY45vyIaw",
    "outputId": "af0906d1-07d6-4a60-da03-ac566b82df7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot go to walk it is raining. I have not found what I am looking for\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def expand_contractions(text):\n",
    "  contractions_pattern = {\n",
    "      r\"(?i)can't\": \"cannot\",\n",
    "      r\"(?i)won't\": \"will not\",\n",
    "      r\"(?i)it's\": \"it is\",\n",
    "      r\"(?i)weren't\": \"were not\",\n",
    "      r\"(?i)I'm\": \"I am\",\n",
    "      r\"(?i)haven't\": \"have not\"\n",
    "  }\n",
    "\n",
    "  for contractions, expansion in contractions_pattern.items():\n",
    "    text = re.sub(contractions, expansion, text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "expanded_text = expand_contractions(txt)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssIASpFzjMY"
   },
   "source": [
    "# **TOKENIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhxfd0Fhzilp",
    "outputId": "408d7faf-6a8f-4b97-cc8f-059804447cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['BGP', 'used', 'for', 'routing', 'within', 'an', 'autonomous', 'system', 'is', 'called', 'Interior', 'Border', 'Gateway', 'Protocol', '(', 'iBGP', ')', '.', 'In', 'contrast', ',', 'the', 'Internet', 'application', 'of', 'the', 'protocol', 'is', 'called', 'Exterior', 'Border', 'Gateway', 'Protocol', '(', 'EBGP', ')', '.']\n",
      "2\n",
      "['BGP used for routing within an autonomous system is called Interior Border Gateway Protocol (iBGP).', 'In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#sample text for tokenization\n",
    "txt = \"BGP used for routing within an autonomous system is called Interior Border Gateway Protocol (iBGP). In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(txt)\n",
    "print(len(words))\n",
    "print(words)\n",
    "\n",
    "# Sentence tokenization\n",
    "\n",
    "sent = sent_tokenize(txt)\n",
    "print(len(sent))\n",
    "print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m07VKPS-urZ"
   },
   "source": [
    "# **Stop words removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WILjBQAU-zfp"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz7HcJt-_BlU"
   },
   "outputs": [],
   "source": [
    "#sample sentence:\n",
    "sentence = \"This is a sample sentence, showing off the stop words filtration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dsfIVwf_LCQ",
    "outputId": "aec2789c-d5e3-433e-9b06-e99cc1d8c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "nltk.download('stopwords')\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Filter stop words\n",
    "new_sentence = [word for word in words if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG9jajwDAClQ",
    "outputId": "9d60efcb-5858-4bd6-fe2b-77f6aea92fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence, showing off the stop words filtration\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4yVHY6_77PC"
   },
   "source": [
    "#**N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itTXSz-E8E-5",
    "outputId": "b3655f54-e838-43cc-ab88-b47a62196a59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lmAhGg682OW",
    "outputId": "db309ddf-57d9-4875-93e4-7a123f96f716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SRv6',), ('is',), ('replacing',), ('MPLS',)]\n",
      "[('SRv6', 'is'), ('is', 'replacing'), ('replacing', 'MPLS')]\n",
      "[('SRv6', 'is', 'replacing'), ('is', 'replacing', 'MPLS')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "  tokens = word_tokenize(text)\n",
    "  n_gram = list(ngrams(tokens, n))\n",
    "  return n_gram\n",
    "\n",
    "txt = \"SRv6 is replacing MPLS\"\n",
    "\n",
    "unigrams = generate_ngrams(txt, 1)\n",
    "bigrams = generate_ngrams(txt, 2)\n",
    "trigrams = generate_ngrams(txt, 3)\n",
    "\n",
    "print(unigrams)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zH97tEIKanYn",
    "Iw9e_OFGayO-",
    "bhv92mThwrK7",
    "pssIASpFzjMY",
    "5m07VKPS-urZ",
    "d4yVHY6_77PC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
