{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install required libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install contractions\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import required libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH97tEIKanYn"
   },
   "source": [
    "## **Case Folding**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q7x74XvagoX"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '3.10.16 (Python 3.10.16)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/Users/marcegar/.pyenv/versions/3.10.16/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "txt = \"Segment routing works either on top of a MPLS network or on an IPv6 network.\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = txt.casefold()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw9e_OFGayO-"
   },
   "source": [
    "# **Special Character Removal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhoyTHXsa2Ej",
    "outputId": "5988b826-2321-4eb3-9981-4ee2aabc6039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#using regular expressions to remove spetial characters\n",
    "clear_str = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", input_str)\n",
    "\n",
    "print(clear_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhrqrFlKb07g",
    "outputId": "ec9d16ce-1111-445a-965f-b665b256b86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "#Labraries in the field of NLP\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Function to clean the string\n",
    "def clean_text(text):\n",
    "  cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "  doc = nlp(cleaned_text)\n",
    "  return ' '.join(token.text for token in doc)\n",
    "\n",
    "# Get the final output\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb-hHuPivNFI",
    "outputId": "ea7feef4-5b32-4975-f062-c436a4eb374e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', '$', '$', '*', 'doing', '?']\n",
      "hello how are you doing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "#Input string\n",
    "input_str = \"hello how are you $$*doing?\"\n",
    "\n",
    "#Tokenize\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "print(tokens)\n",
    "\n",
    "# Remove the special characters\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "clean_str = ' '.join(clean_tokens)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhv92mThwrK7"
   },
   "source": [
    "# **Handling contractions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q41mXNwIxX4U",
    "outputId": "ce987be8-ae1b-44f1-8272-fc25683a8bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot go to walk it is raining. I have not found what I am looking for\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "\n",
    "expanded_txt = contractions.fix(txt)\n",
    "\n",
    "print(expanded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgKpY45vyIaw",
    "outputId": "af0906d1-07d6-4a60-da03-ac566b82df7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot go to walk it is raining. I have not found what I am looking for\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def expand_contractions(text):\n",
    "  contractions_pattern = {\n",
    "      r\"(?i)can't\": \"cannot\",\n",
    "      r\"(?i)won't\": \"will not\",\n",
    "      r\"(?i)it's\": \"it is\",\n",
    "      r\"(?i)weren't\": \"were not\",\n",
    "      r\"(?i)I'm\": \"I am\",\n",
    "      r\"(?i)haven't\": \"have not\"\n",
    "  }\n",
    "\n",
    "  for contractions, expansion in contractions_pattern.items():\n",
    "    text = re.sub(contractions, expansion, text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "txt = \"I can't go to walk it's raining. I haven't found what I'm looking for\"\n",
    "expanded_text = expand_contractions(txt)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssIASpFzjMY"
   },
   "source": [
    "# **TOKENIZATION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhxfd0Fhzilp",
    "outputId": "408d7faf-6a8f-4b97-cc8f-059804447cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['BGP', 'used', 'for', 'routing', 'within', 'an', 'autonomous', 'system', 'is', 'called', 'Interior', 'Border', 'Gateway', 'Protocol', '(', 'iBGP', ')', '.', 'In', 'contrast', ',', 'the', 'Internet', 'application', 'of', 'the', 'protocol', 'is', 'called', 'Exterior', 'Border', 'Gateway', 'Protocol', '(', 'EBGP', ')', '.']\n",
      "2\n",
      "['BGP used for routing within an autonomous system is called Interior Border Gateway Protocol (iBGP).', 'In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sample text for tokenization\n",
    "txt = \"BGP used for routing within an autonomous system is called Interior Border Gateway Protocol (iBGP). In contrast, the Internet application of the protocol is called Exterior Border Gateway Protocol (EBGP).\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(txt)\n",
    "print(len(words))\n",
    "print(words)\n",
    "\n",
    "# Sentence tokenization\n",
    "\n",
    "sent = sent_tokenize(txt)\n",
    "print(len(sent))\n",
    "print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m07VKPS-urZ"
   },
   "source": [
    "# **Stop words removal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz7HcJt-_BlU"
   },
   "outputs": [],
   "source": [
    "#sample sentence:\n",
    "sentence = \"This is a sample sentence, showing off the stop words filtration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dsfIVwf_LCQ",
    "outputId": "aec2789c-d5e3-433e-9b06-e99cc1d8c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "nltk.download('stopwords')\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Filter stop words\n",
    "new_sentence = [word for word in words if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG9jajwDAClQ",
    "outputId": "9d60efcb-5858-4bd6-fe2b-77f6aea92fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence, showing off the stop words filtration\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4yVHY6_77PC"
   },
   "source": [
    "#**N-Grams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itTXSz-E8E-5",
    "outputId": "b3655f54-e838-43cc-ab88-b47a62196a59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lmAhGg682OW",
    "outputId": "db309ddf-57d9-4875-93e4-7a123f96f716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SRv6',), ('is',), ('replacing',), ('MPLS',)]\n",
      "[('SRv6', 'is'), ('is', 'replacing'), ('replacing', 'MPLS')]\n",
      "[('SRv6', 'is', 'replacing'), ('is', 'replacing', 'MPLS')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "  tokens = word_tokenize(text)\n",
    "  n_gram = list(ngrams(tokens, n))\n",
    "  return n_gram\n",
    "\n",
    "txt = \"SRv6 is replacing MPLS\"\n",
    "\n",
    "unigrams = generate_ngrams(txt, 1)\n",
    "bigrams = generate_ngrams(txt, 2)\n",
    "trigrams = generate_ngrams(txt, 3)\n",
    "\n",
    "print(unigrams)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zH97tEIKanYn",
    "Iw9e_OFGayO-",
    "bhv92mThwrK7",
    "pssIASpFzjMY",
    "5m07VKPS-urZ",
    "d4yVHY6_77PC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
