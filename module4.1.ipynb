{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a632af-3bb3-4a4a-8305-b0f94814bc83",
   "metadata": {},
   "source": [
    "# Hands On with Advanced AI & Emerging Applications\n",
    "\n",
    "## Requirementes\n",
    "\n",
    "* Definition of al Large Language Model (LLM)\n",
    "* Definitions of a Prompt\n",
    "\n",
    "## Retrieval Augmented Generation\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a technique that is used in natural language processing to enhance the capabilities of Large Language Models (LLMs) by integrating information retrieval. It improves the responses of LLMs by incorporating updated or user-defined data the LLM was originally not trained on. A RAG system is typically composed of two processes: Indexing and Retrieval + Generation.\n",
    "\n",
    "This module uses the [LangChain](https://www.langchain.com/) to interact with LLMs. Langchain is a framework designed to facititate the development of application powered by LLMs. It provides an abstraction layer by decompossing applications as a set of _chains_ made of prompts, models and data sources. Langchain offers libraries to interact with most of the foundational models available in the market.\n",
    "\n",
    "In this module we will use [Amazon Bedrock](https://aws.amazon.com/bedrock/) which is a fully managed service from AWS that offers access to several foundationsl models. We have chosen [Cloude Sonnet](https://www.anthropic.com/claude/sonnet) from Anthropic as the LLM to use\n",
    "\n",
    "### Setting up connection to Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123fd77e-a99c-4501-a70f-c59669244210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai 1.68.0 requires numpy>=2.0.2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain-community langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd48fc8a-b7b7-4fb2-903f-a13f6847d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c55d523-2a47-419d-9350-ce64f4372cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current President of the United States is Joe Biden. He was inaugurated as the 46th President on January 20, 2021, following his victory in the 2020 presidential election.\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm.invoke(\"Wh is the president of the USA?\")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86dd86d-6561-4ae8-bab3-643174b2bca5",
   "metadata": {},
   "source": [
    "**Note**: See how Claude Sonnet, an LLM released in February 2024, was trained on data that is no longer accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105cb8d-caf4-49ba-a5b3-362e42b8cd2d",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "The indexing phase is responsible for organizing data so that it can be efficiently retrieved by applications powered by large language models (LLMs). To scale Retrieval-Augmented Generation (RAG) systems and handle large datasets, vector databases are commonly used. These databases store information and facilitate efficient data indexing and querying\n",
    "\n",
    "For the purpose of this section we are going to load the content of the markdown file `acme_guidelines.md`. This document has guidelines of the ficitonal companny `ACME` about the IP addresses schemes to be used in the device configuration.\n",
    "\n",
    "\n",
    "LanhChain offers a vast number of classes to load documents, ranging from text files (`.txt`) up to HTML and PDF documents. To load the `acme_guidelines.md` file we are goint ot use the class `TextLoader` from the `langchain.document_loaders` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab76b73-6e60-4700-8108-1533de1c3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"acme_guidelines.md\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f2712-a115-451b-80f3-345b6df3f8bc",
   "metadata": {},
   "source": [
    "#### Splitting\n",
    "As a best practice, data to be stored on the Vertor DB is usually splitted into smaller _chunks_ of data. This makes the _querying_ stage more efficient as only relevant pieces of the data are retrieved. Additionally large chunks may latter not fit in the LLM context window. \n",
    "\n",
    "There are many strategies to split a text. For example, to consider special characters (HTML tags) or punctiation signs. In our example we are using yet another Langchain Class to split a text. The `MarkdownHeaderTextSplitter` allows us to split markdown files based on _Headers_ or _sections_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b985f2f9-05ee-4d92-9085-afc3ef9aa776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Policies', 'Header 2': 'Security protocols'}, page_content='1. The only valid hashing protocol for payload integrity is MD5. This applies for SNMP configurations and authenticated BGP peerings as shown below:  \\n```\\nsnmp-server user MYUSER MYGROUP v3 auth sha <password> priv aes 256 <password>\\n\\n```  \\n2. The only valid authentication protocol for payload encryption is AES 256')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "md_splits = md_splitter.split_text(documents[0].page_content)\n",
    "md_splits[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9aeb2-6f74-40ba-bf44-99e2da787ecd",
   "metadata": {},
   "source": [
    "Another alternative to split text files is to use the class `RecursiveCharacterTextSplitter`, which allows us to define separators between chunks, the size of each chunk and the number of items being overlapped between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b56e3f8-aaf1-4cf4-a1b1-82b9d3f2b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\"], \n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e3aac-6a5f-438a-8972-7b3bf5ab2820",
   "metadata": {},
   "source": [
    "At this point we can test the content of a chunk by displaying the attribute `page_content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b3aeb9-fe82-4c3d-9ffc-43c2b0adee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. IP Addresses assigned to Router IDs (RID) of routing protocols such as OSPF, EIGRP or BGP must be taken from the CIDR 172.20.0.0/16\\n\\n    Example: \\n    This is a wrong interface configuration'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[3].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d57a6-0591-4364-a3b9-9eb6f864aa60",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "The next and last stage in the _Indexing_ process is to actually store the chunks in a Vector DB\n",
    "\n",
    "As the name implies, a Vector DB saved data as **vectors** not as raw text, therefore and embedding function must transform thus chunks into vectors. In this task we are using an _In-Memory_ Database and [Amazon Titan Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6940f74-618c-4ac3-bc17-7a86127dcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_aws import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "400f0849-c191-4c00-8b39-5c994f383de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f06a289-d366-4ad7-875c-0ba7227f6691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e07a3ebc-1273-462a-8ba8-1bdd65ea9809',\n",
       " 'bf008a7c-dcab-42ee-93f3-9cb8ee6aca04',\n",
       " '8babc2bd-ffba-49e4-9df6-10a9000dd7e8',\n",
       " '4f99c050-3605-48e0-846a-f9cc9771f2a3']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(documents=md_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db6439-ed9e-4656-8530-30184a84c601",
   "metadata": {},
   "source": [
    "Note that we have used the variable `md_splits` which has the chunks generated by the `MarkdownHeaderTextSplitter`\n",
    "\n",
    "Let us examine the first two entries on the Vector DB. Note how the dimension of the embeddings in 1024. Only the first three items of each vector are being displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e83af93-22fa-417c-8b46-2e47f2e915a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e07a3ebc-1273-462a-8ba8-1bdd65ea9809: [-0.027660029008984566, 0.06223149225115776, -0.004787181504070759] (1024)\n",
      "bf008a7c-dcab-42ee-93f3-9cb8ee6aca04: [-0.06255853921175003, 0.015993745997548103, 0.0010123404208570719] (1024)\n"
     ]
    }
   ],
   "source": [
    "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "    if index < 2:\n",
    "        print(f\"{id}: {doc['vector'][0:3]} ({len(doc['vector'])})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b68b7d-9fcd-4fb7-b165-c23cb05925be",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "Before actually implement the retrieval stage with the LLM, let us test the _querying_ of the vector VB with the method `similarity_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64ef27ba-b524-4dcb-942b-6b76a6682f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The only valid hashing protocol for payload integrity is MD5. This applies for SNMP configurations and authenticated BGP peerings as shown below:  \n",
      "```\n",
      "snmp-server user MYUSER MYGROUP v3 auth sha <password> priv aes 256 <password>\n",
      "\n",
      "```  \n",
      "2. The only valid authentication protocol for payload encryption is AES 256\n",
      "The following document states the configuration policies of the devices of the ACME company\n"
     ]
    }
   ],
   "source": [
    "query = \"approved security mechanisms\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06019dc-aa6f-49c4-ab15-f129ecbdf729",
   "metadata": {},
   "source": [
    "**Note:** See how the similary search takes into account semantic meaning of the text rather than exact words or grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac728116-f0d9-4a3b-a83d-1331e1521d88",
   "metadata": {},
   "source": [
    "### Retrieval Using LangChain LCEL\n",
    "\n",
    "In this task, we are going to use LangChain constructs, namely _chains_,  to test the capabilities of the RAG system. As a first step we will invoke the LLMs without using RAG just to realize the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "335471de-0650-4dff-a176-b5facf353696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "query = \"What IPs are allowed in Ethernet Interfaces?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc596ff8-75bb-4ced-aa1f-baabf952133e",
   "metadata": {},
   "source": [
    "#### Query without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d297034f-59ff-49d4-ae43-b88618d7c38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# IP Addresses Allowed in Ethernet Interfaces\\n\\nEthernet interfaces can be configured with any valid IP address from the following address spaces:\\n\\n## IPv4\\n- Any address in the range 0.0.0.0 to 255.255.255.255, except for:\\n  - Reserved addresses (e.g., 127.0.0.0/8 for loopback)\\n  - Multicast addresses (224.0.0.0/4)\\n  - Broadcast addresses (like 255.255.255.255)\\n  - Other special-purpose addresses\\n\\n## IPv6\\n- Any valid IPv6 address, with common types including:\\n  - Global unicast addresses (2000::/3)\\n  - Link-local addresses (fe80::/10)\\n  - Unique local addresses (fc00::/7)\\n\\nThe specific IP addresses you should use depend on your network requirements, including whether the interface is for a LAN, WAN connection, or specialized network configuration.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = PromptTemplate.from_template(template = query) |  llm\n",
    "result = chain.invoke(input={})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442d52f-c347-417f-8408-727742fa0884",
   "metadata": {},
   "source": [
    "See how using basic prompt and invoking the LLM directly, the generated response is very generic. The LLM claims that any IP address can be used on a Ethernet Interface\n",
    "\n",
    "#### Querying with RAG\n",
    "\n",
    "The idea behind RAG is to provider more context to LLM, so it was use it to generate a response. In our current example the context are the Policies listed in the `acme_guidelines.md` document. We will use a  prompt that apart from specify the question to the LLM also give context to it\n",
    "```\n",
    "HUMAN\n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7629c044-e64b-40b1-b1f9-62698077766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgomezve/Documents/Development/ai/neai_module_4/venv/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "retrieval_qa_chat_prompt.messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2cb766-a2a8-4386-aa38-e886aa3c1aa2",
   "metadata": {},
   "source": [
    "Note how the `rag_chain` is made from three components:\n",
    "\n",
    "1. Two parallel branch to get (a) the `context` and (b) `the question`\n",
    "2. The RAG promp\n",
    "3. The LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "104ae0ce-a8ab-4c18-b385-b0a68fcb8bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, IP addresses on Ethernet interfaces must be taken from the CIDR 10.1.0.0/8. This is shown in the example where a valid interface configuration uses an IP address of 10.1.1.254/24, and another example shows a routed interface with IP address 10.1.2.254/24.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "rag_chain = (\n",
    "    {'context': vector_store.as_retriever() | format_docs, 'question' : RunnablePassthrough()} |\n",
    "    retrieval_qa_chat_prompt |\n",
    "    llm\n",
    ")\n",
    "result = rag_chain.invoke(\"What IPs are allowed on Ethernet Interfaces\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b03fa-1b6a-4f93-9442-76fc3a19c621",
   "metadata": {},
   "source": [
    "See how in this execution the LLM considered the content of the `acme_guidelines.md` markdown file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c6aef-5323-4a58-b011-5502b8435b18",
   "metadata": {},
   "source": [
    "### Retrieval Using LangGraph\n",
    "\n",
    "In this task, we are going to use [LangGraph](https://www.langchain.com/langgraph) constructs, namely Nodes and Edges, to test the capabilities of the RAG\n",
    "\n",
    "**Note**: This task asummes that the in-memory VectorDB has already been indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32efbc1f-57bb-4eef-8ac3-f770a680f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langgraph graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291975b-48f8-4f20-991b-f6593ace9b10",
   "metadata": {},
   "source": [
    "We start defining the _State_ of the grpah. The _State_ is the data being exchanged by the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ef56b66-6cb0-45eb-8614-dfaa813be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94ca40-8a00-4cfa-9e1b-11667c5801fd",
   "metadata": {},
   "source": [
    "In our use-case we define two nodes: \n",
    "\n",
    "*  Retrieve: It queries the Vector DB using limiraty search from the input question\n",
    "*  Generate. It leverages the RAG prompt to give context to the LLM and expand its scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09a5524f-841e-4b61-88bd-a8be255e9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"],k=2)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = retrieval_qa_chat_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66350297-4271-4788-83e4-dc3ccc471f76",
   "metadata": {},
   "source": [
    "Then we build our graph by defining node and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "def81e6b-8e84-45d1-ba44-d73001acac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d3ff0df-9ca5-4de6-9cff-ef2cee244b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2c769-b166-4f90-b048-69460d030446",
   "metadata": {},
   "source": [
    "Finally we invoke our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "834948e6-284c-4368-bcad-7863fc314724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ethernet interfaces should use IP addresses from the CIDR 10.1.0.0/8 range. This is specified in the context which states that routable addresses used on physical interfaces (including Ethernet interfaces) must be taken from this range. The example shows a valid configuration with IP address 10.1.1.254/24.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What IPs are allowed on Ethernet Interfaces\"})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bef72c1-e512-4372-99d7-2df6891b56e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bf008a7c-dcab-42ee-93f3-9cb8ee6aca04', metadata={'Header 1': 'Policies', 'Header 2': 'Addressing Schema'}, page_content='1. Routable addresses used on physical Interfaces (E.g: Ethernet 1/2), Port-Chanels (port-channel1) or SVI Interfaces must be taken from the CIDR 10.1.0.0/8  \\nExample:\\nThis is a valid interface configuration\\n```\\ninterface vlan 123\\nno switchport\\nip address 10.1.1.254/24\\n```  \\n2. IP Addresses assigned to Router IDs (RID) of routing protocols such as OSPF, EIGRP or BGP must be taken from the CIDR 172.20.0.0/16  \\nExample:\\nThis is a valid OSPF process configuration  \\n```\\nrouter ospf OSPF1\\nrouter-id  172.1.1.5\\n```'),\n",
       " Document(id='8babc2bd-ffba-49e4-9df6-10a9000dd7e8', metadata={'Header 1': 'Policies', 'Header 2': 'Interface description'}, page_content='1. Interfaces must have a description that matches the following convention: <type>_<speed>_<peer_id>, where `type` is `L2` for switches interfaces, `L3` for routed interfaces and `PC` to interfaces member of a port-channel  \\n```\\ninterface ethernet1/1\\nno switchport\\ndescription L3_10G_DMZ Router\\nip address 10.1.2.254/24\\n```')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f631c29-a6a8-4785-a378-18c72e4d1547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
